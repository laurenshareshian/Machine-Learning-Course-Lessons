{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "---\n",
    "<a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "The above lessons were meant to get you comfortable with different types of machine learning algorithms. However, in practice, we would never use our entire dataset to train our model. Instead, we would use a portion of our data, the training set, to train the data, and then we would evaluate the accuracy of our model on the testing portion of our dataset. Since the test portion was not used to train the model, it gives us a more honest indication of how well our model does at predicting new data it hasn't encountered before. There are a few techniques for training and testing. This first one is less computationally intensive.\n",
    "\n",
    "### Technique 1: Train/Validate/Test\n",
    "\n",
    "First, a couple of vocab words:\n",
    "\n",
    "**Training Dataset**: The sample of data used to fit the model. The actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "\n",
    "**Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly.\n",
    "\n",
    "**Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner).\n",
    "\n",
    "<img src=\"images/train.png\" width=\"400\">\n",
    "\n",
    "Side note: there is no hard and fast rule about how to proportion your data. Just know that your model is limited in what it can learn if you limit the data you feed it. However, if your test set is too small, it won’t provide an accurate estimate as to how your model will perform. \n",
    "\n",
    "### Boston Example\n",
    "\n",
    "Scikit-learn has many data sets built in that you can use. Let's load in a Boston dataset of housing info and housing prices. When you first load the data, it's a bit hard to understand what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "boston.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can read the docs to understand better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "Load and return the boston house-prices dataset (regression).\n",
      "\n",
      "    ==============     ==============\n",
      "    Samples total                 506\n",
      "    Dimensionality                 13\n",
      "    Features           real, positive\n",
      "    Targets             real 5. - 50.\n",
      "    ==============     ==============\n",
      "\n",
      "    Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    data : Bunch\n",
      "        Dictionary-like object, the interesting attributes are:\n",
      "        'data', the data to learn, 'target', the regression targets,\n",
      "        'DESCR', the full description of the dataset,\n",
      "        and 'filename', the physical location of boston\n",
      "        csv dataset (added in version `0.20`).\n",
      "\n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed a wrong data point at [445, 0].\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_boston\n",
      "    >>> boston = load_boston()\n",
      "    >>> print(boston.data.shape)\n",
      "    (506, 13)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)\n",
    "print(load_boston.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that all of the predictor variables are contained in data and the target variable (housing price) is contained in the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test/Train sets\n",
    "Let's omit the validation set for now and focus on splitting into training and testing sets. If we save 30% for the testing set and run a simple linear regression, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7609419100141347\n",
      "Test: 0.6855557785812373\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit the model against the training data\n",
    "lr2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model against the testing data\n",
    "print('Train:', lr2.score(X_train, y_train))\n",
    "print('Test:', lr2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell multiple times to notice the variation. Notice the R^2 value of the test (hold-out) set. Notice that model performance is usually a little lower on the test set. This is expected. In fact, this lower value is a much more accurate number to report as \"real world\" performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Cross Validation\n",
    "\n",
    "Cross validation assigns a certain percentage of the dataset to test data, and then does this multiple times. \n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on the training set and testing the analysis on the other subset. To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance.\n",
    "\n",
    "The upside of this method is it avoids unluckily sampling one unrepresentative set of test data. However, the downside is this method is computationally intensive. This method works great on small to medium-sized datasets. This is absolutely not the kind of thing you’d want to try on a massive dataset. \n",
    "\n",
    "<img src=\"images/train3.png\" width=\"400\">\n",
    "\n",
    "Not surprisingly, scikit-learn can help us do this using a method called cross_val_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.46030057 26.04862111 33.07413798 80.76237112 33.31360656]\n",
      "Average MSE:  37.13180746769922\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(lr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# scores output is negative, which is because\n",
    "# Scikit-learn uses negative mean squared error so that \n",
    "# scores always improve with higher values \n",
    "print(-scores)\n",
    "print('Average MSE: ', np.mean(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of a **train/test split:**\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "- Simpler to examine the detailed results of the testing process\n",
    "\n",
    "Advantages of **cross validation:**\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "Recommendations for **cross validation:**\n",
    "\n",
    "- K can be any number, but K=4 or 5 is common\n",
    "\n",
    "- Each response class should be represented with equal proportions in each of the K folds\n",
    "\n",
    "- Scikit-learn's cross_val_score function does this by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Validation Sets to Tune Hyperparameters: Test/Train/Split\n",
    "In the above linear regression model, we were not tuning any hyperparameters. In the Ridge and Lasso examples below, we would like to tune alpha, so we will create a validation set.\n",
    "\n",
    "To create a validation set, we'll need to use test/train/split twice, once to separate the test set and then again to separate the validation set. We'll separate the data into 60% training, 20% validation, and 20% testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.6% | validation: 0.2% | test 0.2%\n"
     ]
    }
   ],
   "source": [
    "# intermediate/test split (gives us test set)\n",
    "X_intermediate, X_test, y_intermediate, y_test = train_test_split(X, \n",
    "                                                                  y, \n",
    "                                                                  shuffle=True,\n",
    "                                                                  test_size=0.2, \n",
    "                                                                  random_state=15)\n",
    "\n",
    "# train/validation split (gives us train and validation sets)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n",
    "                                                                y_intermediate,\n",
    "                                                                shuffle=False,\n",
    "                                                                test_size=0.25,\n",
    "                                                                random_state=2018)\n",
    "# delete intermediate variables\n",
    "del X_intermediate, y_intermediate\n",
    "\n",
    "# print proportions\n",
    "print('train: {}% | validation: {}% | test {}%'.format(round(len(y_train)/len(y),2),\n",
    "                                                       round(len(y_validation)/len(y),2),\n",
    "                                                       round(len(y_test)/len(y),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that regularization is a form of constrained optimization that imposes limits on determining model parameters. It effectively allows us to add bias to a model that’s overfitting (overfitting occurs when your model is so specific to your training dataset that it does worse in predicting your test data). We can control the amount of bias with a hyperparameter called lambda (called alpha in Python since lambda is a reserved word) that defines regularization strength. Let's see below how we can test different values of alpha against our validation set in order to get the best hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error\n",
      "----------------------------------------------------------------------------\n",
      "alpha:   0.001 | train error: 22.924 | val error: 19.804 | test error: 23.958\n",
      "alpha:    0.01 | train error: 22.924 | val error: 19.801 | test error: 23.943\n",
      "alpha:     0.1 | train error: 22.938 | val error: 19.791 | test error: 23.82\n",
      "alpha:       1 | train error: 23.315 | val error: 20.158 | test error: 23.533\n",
      "alpha:      10 | train error: 24.199 | val error: 20.981 | test error: 23.369\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
    "print('Mean Squared Error')\n",
    "print('-'*76)\n",
    "for alpha in alphas:\n",
    "    # instantiate and fit model\n",
    "    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    # calculate errors\n",
    "    new_train_error = mean_squared_error(y_train, ridge.predict(X_train))\n",
    "    new_validation_error = mean_squared_error(y_validation, ridge.predict(X_validation))\n",
    "    new_test_error = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "    # print errors as report\n",
    "    print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n",
    "          format(alpha,\n",
    "                 round(new_train_error,3),\n",
    "                 round(new_validation_error,3),\n",
    "                 round(new_test_error,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few key takeaways here. First, notice the U-shaped behavior exhibited by the validation error above. It starts at 19.796, goes down for two steps and then back up. Also notice that validation error and test error tend to move together, but by no means is the relationship perfect. We see both errors decrease as alpha increases initially but then test error keeps going down while validation error rises again. \n",
    "\n",
    "The U shape is typical. We prefer to be somewhere in the sweet spot between not overfitting (having too little error in the training data) and not underfitting (having too much error in the training data).\n",
    "\n",
    "\n",
    "<img src=\"images/train2.png\" width=\"400\">\n",
    "\n",
    "In our case, we'll decide to use alpha=0.1 to now train our entire training dataset since this gave us the smallest validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train error: 21.87892746431845 R2 train:  0.7454491108712407\n",
      "MSE test error: 23.679851355216798 R2 test:  0.6937869418612654\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15)\n",
    "\n",
    "# instantiate model\n",
    "ridge = Ridge(alpha=0.1, fit_intercept=True, random_state=99)\n",
    "#fit model\n",
    "ridge.fit(X_train, y_train)\n",
    "#evaluate model\n",
    "new_train_error = mean_squared_error(y_train, ridge.predict(X_train))\n",
    "new_test_error = mean_squared_error(y_test, ridge.predict(X_test))\n",
    "print('MSE train error:', new_train_error, 'R2 train: ', ridge.score(X_train, y_train))\n",
    "print('MSE test error:', new_test_error, 'R2 test: ', ridge.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can conclude that 69.4% of the variation in home price in our test set can be attributed to variation in our explanatory variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Validation Sets to Tune Hyperparameters: Cross Validation & KFold\n",
    "\n",
    "Cross validation assigns a certain percentage of the dataset to test data, and then does this multiple times. Using `cross_val_score`, is convenient, but it doesn't allow features to be manipulated during the cross validation steps. An alternative is to use `KFold`, which will allow manipulation during cross validation.\n",
    " Just for fun, we'll also use a different model called LASSO instead of Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha:  0.001 | mean(train_error):  21.819 | mean(val_error): 23.3657\n",
      "alpha:   0.01 | mean(train_error): 21.8551 | mean(val_error): 23.4134\n",
      "alpha:    0.1 | mean(train_error): 22.9668 | mean(val_error): 24.5981\n",
      "alpha:      1 | mean(train_error):  26.734 | mean(val_error): 28.2354\n",
      "alpha:     10 | mean(train_error):  40.183 | mean(val_error): 40.9859\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "for alpha in alphas:\n",
    "    train_errors = []\n",
    "    validation_errors = []\n",
    "    for train_index, val_index in kf.split(X, y):\n",
    "        \n",
    "        # split data\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        # instantiate model\n",
    "        lasso = Lasso(alpha=alpha, fit_intercept=True, random_state=77)\n",
    "        # fit model\n",
    "        lasso.fit(X_train, y_train)\n",
    "        \n",
    "        #calculate errors\n",
    "        train_error = mean_squared_error(y_train, lasso.predict(X_train))\n",
    "        val_error = mean_squared_error(y_val, lasso.predict(X_val))\n",
    "\n",
    "        # append to appropriate list\n",
    "        train_errors.append(train_error)\n",
    "        validation_errors.append(val_error)\n",
    "    \n",
    "    # generate report\n",
    "    print('alpha: {:6} | mean(train_error): {:7} | mean(val_error): {}'.\n",
    "          format(alpha,\n",
    "                 round(np.mean(train_errors),4),\n",
    "                 round(np.mean(validation_errors),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we'll decide to use alpha=0.001 to now train our entire training dataset since this gave us the smallest validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train error: 21.87195567342094 R2 train:  0.7455302243341687\n",
      "MSE test error: 23.774056435209108 R2 test:  0.6925687405641405\n"
     ]
    }
   ],
   "source": [
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=15)\n",
    "\n",
    "# instantiate model\n",
    "lasso = Lasso(alpha=0.001, fit_intercept=True, random_state=77)\n",
    "# fit model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "#evaluate model\n",
    "new_train_error = mean_squared_error(y_train, lasso.predict(X_train))\n",
    "new_test_error = mean_squared_error(y_test, lasso.predict(X_test))\n",
    "print('MSE train error:', new_train_error, 'R2 train: ', lasso.score(X_train, y_train))\n",
    "print('MSE test error:', new_test_error, 'R2 test: ', lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can conclude that 69.3% of the variation in home price in our test set can be attributed to variation in our explanatory variables, which is slightly worse than the 69.4% we got before from Ridge regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

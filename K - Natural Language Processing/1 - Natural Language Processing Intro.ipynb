{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "Run the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863\n",
    "\n",
    "https://www.freecodecamp.org/news/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3/\n",
    "\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "NLP is a branch of artificial intelligence that deals with analyzing, understanding and generating the languages that humans use naturally in order to interface with computers in both written and spoken contexts using natural human languages instead of computer languages.\n",
    "\n",
    "### Applications of NLP\n",
    "\n",
    "- Machine translation(Google Translate)\n",
    "- Natural language generation\n",
    "- Web Search\n",
    "- Spam filters\n",
    "- Sentiment Analysis (positive or negative tone)\n",
    "- Chatbots\n",
    "\n",
    "… and many more\n",
    "\n",
    "### Preprocessing of data\n",
    "\n",
    "A **text corpus** is a large and structured set of texts. \n",
    "\n",
    "Here's an example of a corpus. This example is a document containing three sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = 'I like football. Football is one of my favorite sports. I play Fantasy Football with my friends. I am in several different fantasy football leagues.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply our machine learning algorithms, we often preprocess the corpus in order to transform the raw data in a useful and efficient format. Here are some common types of preprocessing:\n",
    "\n",
    "- **Normalization**: Making all the text lower case is one of the simplest and most effective forms of text preprocessing.\n",
    "\n",
    "We'll do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like football. football is one of my favorite sports. i play fantasy football with my friends. i am in several different fantasy football leagues.\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Punctuation removal**: Punctuation can also be removed. The string library contains a list of (most) punctuation characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll remove the punctuation using regular expressions. Regular expressions are a topic unto themselves and you can google tutorials on them if you'd like. We'll import the regular expression package (called re) in order to use them. Don't worry about the code in the following cell for now, just run it to remove the punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like football  football is one of my favorite sports  i play fantasy football with my friends  i am in several different fantasy football leagues \n"
     ]
    }
   ],
   "source": [
    "punc_re = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "corpus = map(lambda x: punc_re.sub(' ', x), corpus)\n",
    "corpus = ''.join(list(corpus))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Stop words** are common words that do not contribute much of the information in a text document. Words like ‘the’, ‘is’, ‘a’ have less value and add noise to the text data. Here are the first ten contained in the nltk stopwords list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove them from our corpus now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like football football one favorite sports play fantasy football friends several different fantasy football leagues\n"
     ]
    }
   ],
   "source": [
    "resultwords  = [word for word in corpus.split() if word not in stopwords.words('english')]\n",
    "corpus = ' '.join(resultwords)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tokenization** is the process of breaking up a text document into individual words called tokens. Let's do that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'football',\n",
       " 'football',\n",
       " 'one',\n",
       " 'favorite',\n",
       " 'sports',\n",
       " 'play',\n",
       " 'fantasy',\n",
       " 'football',\n",
       " 'friends',\n",
       " 'several',\n",
       " 'different',\n",
       " 'fantasy',\n",
       " 'football',\n",
       " 'leagues']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other types of preprocessing that we do now but are helpful to know about are:\n",
    "    \n",
    "- **Stemming** is the process of reducing a word to its stem/root word. It reduces inflection in words (e.g. ‘help’, ’helping’, ’helped’, ’helpful’) to their root form (e.g. ‘help’). It removes the morphological affixes from words, leaving only the word stem. The stem word may or may not be a valid word in the language. For example ‘movi’ is the root word for ‘movie’, ‘emot’ is the root word for ‘emotion’.\n",
    "\n",
    "- **Lemmatization** does the same thing as stemming, converting a word to its root form but with one difference i.e., the root word in this case belongs to a valid word in the language. For example the word caring would map to ‘care’ and not ‘car’ as the in case of stemming.\n",
    "\n",
    "- **Ngrams** are the combination of multiple words used together. N-grams can be used when we want to preserve sequence information in the document, like what word is likely to follow the given one. For example, if we were making a Donald Trump chatbot, we might want the word \"news\" to always follow the word \"fake\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Vectorization\n",
    "\n",
    "Once the data is preprocessed, we can numerically represent text data. Here are some ways to do so.\n",
    "\n",
    "\n",
    "- **Bag of Words** we can think of as creating a table where columns are the set of unique words in the corpus and rows correspond to each sentence(document). We set the value as 1 if the word is present in the sentence else we set it to 0. Consider the list below as two documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The car is driven on the road.', \n",
    "          'The truck is driven on the highway.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the NLTK count vectorizer to create a bag of words, where each row corresponds to a different document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>driven</th>\n",
       "      <th>highway</th>\n",
       "      <th>is</th>\n",
       "      <th>on</th>\n",
       "      <th>road</th>\n",
       "      <th>the</th>\n",
       "      <th>truck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   car  driven  highway  is  on  road  the  truck\n",
       "0    1       1        0   1   1     1    2      0\n",
       "1    0       1        1   1   1     0    2      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TF-IDF** stands for Term Frequency - Inverse Document Frequency. It takes into account that we should weight rare words more highly than common words.\n",
    "\n",
    "**Term Frequency** defines the probability of finding a word in the document. Let’s say we want to find what is the probability of finding $\\text{word}_i$ in $\\text{document}_j$:\n",
    "\n",
    "$\\text{ TermFrequency(word_i,document_j}) = \\frac{\\text{Number of times word_i occurs in document_j}}{\\text{Total number of words in document_j}}$\n",
    "\n",
    "**Inverse Document Frequency**:The intuition behind IDF is that a word is not of much use if it is appearing in all the documents. It defines how unique the word is in the total corpus:\n",
    "\n",
    "$\\text{ InverseDocumentFrequency(word_i,All Documents in Corpus}) = \\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})$\n",
    "\n",
    "If word_i is more frequent in the corpus then IDF value decreases.\n",
    "\n",
    "If word_i is not frequent which means ni decreases and hence IDF value increases.\n",
    "\n",
    "And finally, we obtain the formula for TF-IDF:\n",
    "\n",
    "$\\text{TF-IDF = TF(word_i, document_j) * IDF(word_i, All documents in corpus)}$\n",
    "\n",
    "We can calculate the TF-IDF matrix in the above example:\n",
    "\n",
    "<img src=\"images/td.png\" width=500>\n",
    "\n",
    "From the above table, we can see that TF-IDF of common words was zero, which shows they are not significant. On the other hand, the TF-IDF of “car” , “truck”, “road”, and “highway” are non-zero. These words have more significance.\n",
    "\n",
    "You will see the matrix written in this form:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>the</th>\n",
       "      <th>car</th>\n",
       "      <th>truck</th>\n",
       "      <th>is</th>\n",
       "      <th>driven</th>\n",
       "      <th>on</th>\n",
       "      <th>the</th>\n",
       "      <th>road</th>\n",
       "      <th>highway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   the    car  truck   is  driven   on  the   road  highway\n",
       "A  0.0  0.043  0.000  0.0     0.0  0.0  0.0  0.043    0.000\n",
       "B  0.0  0.000  0.043  0.0     0.0  0.0  0.0  0.000    0.043"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(0,0),(0.043,0),(0,0.043),(0,0),(0,0),(0,0),(0,0),(0.043,0),(0,0.043)]\n",
    "(pd.DataFrame(data, columns=['A','B'], index=['the', 'car', 'truck', 'is', 'driven', 'on', 'the', 'road', 'highway'])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will let Python create the TF-IDF matrix for us instead.\n",
    "\n",
    "Nicely enough, the sklearn TF-IDF vectorizer can make the corpus lowercase and remove punctuation and stop words. We remove punctuation using the argument stop_words by setting it equal to a regular expression that indicates we only want alphabetic characters and not punctuation. We can also add a min_df argument, which ignores terms in the document that have a document frequency strictly lower than the given threshold.\n",
    "\n",
    "Don't be alarmed that sklearn's matrix looks quite a bit different than yours. The values differ slightly because sklearn uses a smoothed version idf and various other little optimizations. For example, sklearn uses $\\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})+1$ instead of just $ \\log(\\frac{\\text{Total number of documents}}{\\text{Number of documents which contain word_i}})$ to calculate the IDF score. This ensures that the words with an IDF score of zero (i.e., words that occur in every document) don’t get suppressed entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car</th>\n",
       "      <th>driven</th>\n",
       "      <th>highway</th>\n",
       "      <th>road</th>\n",
       "      <th>truck</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.449436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449436</td>\n",
       "      <td>0.631667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        car    driven   highway      road     truck\n",
       "0  0.631667  0.449436  0.000000  0.631667  0.000000\n",
       "1  0.000000  0.449436  0.631667  0.000000  0.631667"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = TfidfVectorizer(lowercase=True, \n",
    "                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                     stop_words=stopwords.words('english'),\n",
    "                     min_df=1)\n",
    "\n",
    "X = tf.fit_transform(corpus)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=tf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "Latent Semantic Analysis is just SVD applied to a word/document matrix\n",
    "\n",
    "- D1 = \"I like databases\"\n",
    "- D2 = \"I hate databases\"\n",
    "\n",
    "then the document-term matrix would be:\n",
    "\n",
    "$$ \\begin{matrix} \\text{I} & \\text{like} & \\text{hate} & \\text{database} \\\\ 1 & 1 & 0 & 1 \\\\ 1 & 0 & 1 & 1  \\end{matrix} $$\n",
    "\n",
    "With each row being a different document\n",
    "and each column being a new word.\n",
    "\n",
    "In this case our decomposition has a new interpretation:\n",
    "- $\\Sigma$ are the importances of each of our topics\n",
    "- $U$ is a transform from a word vector to the topics that word is most used in\n",
    "- $V$ is a transform from each document to the topics it is about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform LSA on a list of six documents below. We'll first create a TF-IDF matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>basketball</th>\n",
       "      <th>broncos</th>\n",
       "      <th>cowboys</th>\n",
       "      <th>cubs</th>\n",
       "      <th>football</th>\n",
       "      <th>giants</th>\n",
       "      <th>hendrix</th>\n",
       "      <th>jagger</th>\n",
       "      <th>jam</th>\n",
       "      <th>joplin</th>\n",
       "      <th>pearl</th>\n",
       "      <th>pop</th>\n",
       "      <th>prince</th>\n",
       "      <th>redsox</th>\n",
       "      <th>rock</th>\n",
       "      <th>stars</th>\n",
       "      <th>tigers</th>\n",
       "      <th>tupac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.479185</td>\n",
       "      <td>0.675356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.397106</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.559675</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.464579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.609819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.506202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.479185</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560603</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675356</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.451635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544082</td>\n",
       "      <td>0.451635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.544082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.570997</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.461804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.461804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   baseball  basketball   broncos   cowboys      cubs  football    giants  \\\n",
       "0  0.479185    0.675356  0.000000  0.000000  0.000000  0.560603  0.000000   \n",
       "1  0.397106    0.000000  0.000000  0.000000  0.559675  0.000000  0.559675   \n",
       "2  0.000000    0.000000  0.609819  0.609819  0.000000  0.506202  0.000000   \n",
       "3  0.479185    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "5  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "6  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    hendrix    jagger       jam    joplin     pearl       pop    prince  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.451635  0.000000  0.000000  0.000000  0.000000  0.544082  0.451635   \n",
       "5  0.473977  0.570997  0.000000  0.000000  0.000000  0.000000  0.473977   \n",
       "6  0.000000  0.000000  0.461804  0.461804  0.461804  0.000000  0.000000   \n",
       "\n",
       "     redsox      rock     stars    tigers     tupac  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.464579  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.560603  0.000000  0.000000  0.675356  0.000000  \n",
       "4  0.000000  0.000000  0.544082  0.000000  0.000000  \n",
       "5  0.000000  0.473977  0.000000  0.000000  0.000000  \n",
       "6  0.000000  0.383337  0.000000  0.000000  0.461804  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ['Football baseball basketball',\n",
    "            'baseball giants cubs redsox',\n",
    "            'football broncos cowboys',\n",
    "            'baseball redsox tigers',\n",
    "            'pop stars hendrix prince',\n",
    "            'hendrix prince jagger rock',\n",
    "            'joplin pearl jam tupac rock',\n",
    "          ]\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=True, \n",
    "                     token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                     stop_words=stopwords.words('english'),\n",
    "                     min_df=1)\n",
    "\n",
    "X = vectorizer.fit_transform(example)\n",
    "\n",
    "pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the words that are contained in the columns here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball',\n",
       " 'basketball',\n",
       " 'broncos',\n",
       " 'cowboys',\n",
       " 'cubs',\n",
       " 'football',\n",
       " 'giants',\n",
       " 'hendrix',\n",
       " 'jagger',\n",
       " 'jam',\n",
       " 'joplin',\n",
       " 'pearl',\n",
       " 'pop',\n",
       " 'prince',\n",
       " 'redsox',\n",
       " 'rock',\n",
       " 'stars',\n",
       " 'tigers',\n",
       " 'tupac']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll apply SVD using 2 components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseball</th>\n",
       "      <th>basketball</th>\n",
       "      <th>broncos</th>\n",
       "      <th>cowboys</th>\n",
       "      <th>cubs</th>\n",
       "      <th>football</th>\n",
       "      <th>giants</th>\n",
       "      <th>hendrix</th>\n",
       "      <th>jagger</th>\n",
       "      <th>jam</th>\n",
       "      <th>joplin</th>\n",
       "      <th>pearl</th>\n",
       "      <th>pop</th>\n",
       "      <th>prince</th>\n",
       "      <th>redsox</th>\n",
       "      <th>rock</th>\n",
       "      <th>stars</th>\n",
       "      <th>tigers</th>\n",
       "      <th>tupac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>component_1</th>\n",
       "      <td>0.59434</td>\n",
       "      <td>0.26389</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.10775</td>\n",
       "      <td>0.25565</td>\n",
       "      <td>0.30849</td>\n",
       "      <td>0.25565</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.47627</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.31811</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>component_2</th>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.51977</td>\n",
       "      <td>0.33357</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.10539</td>\n",
       "      <td>0.29259</td>\n",
       "      <td>0.51977</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.36438</td>\n",
       "      <td>0.29259</td>\n",
       "      <td>-0.00000</td>\n",
       "      <td>0.10539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             baseball  basketball  broncos  cowboys     cubs  football  \\\n",
       "component_1   0.59434     0.26389  0.10775  0.10775  0.25565   0.30849   \n",
       "component_2  -0.00000     0.00000  0.00000  0.00000  0.00000   0.00000   \n",
       "\n",
       "              giants  hendrix   jagger      jam   joplin    pearl      pop  \\\n",
       "component_1  0.25565  0.00000  0.00000  0.00000  0.00000  0.00000  0.00000   \n",
       "component_2  0.00000  0.51977  0.33357  0.10539  0.10539  0.10539  0.29259   \n",
       "\n",
       "              prince   redsox     rock    stars   tigers    tupac  \n",
       "component_1  0.00000  0.47627  0.00000  0.00000  0.31811  0.00000  \n",
       "component_2  0.51977 -0.00000  0.36438  0.29259 -0.00000  0.10539  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(2)\n",
    "X_svd = svd.fit_transform(X)\n",
    "\n",
    "pd.DataFrame(svd.components_.round(5),\n",
    "             index = [\"component_1\",\"component_2\"],\n",
    "             columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that the first component accounted for 14% of the explained variance and the second component accounted from 16%. It may be confusing that these are not in descending order. It is due to a rather obscure fact that we didn't apply the standard scaler to our data first. However, we'll keep our data unscaled in order to more easily interpret later results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14219813, 0.16486574])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also want to scale our results using Normalizer. This ensures that each vector has a norm of 1. Vectors with a norm of 1 are easy to work with for calculating similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_svd = Normalizer(copy=False).fit_transform(X_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each document is a linear combination of the PCA components. We notice that the first sports-related documents are composed entirely of the first component. The music-related documents are composed entirely of the second component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>component_1</th>\n",
       "      <th>component_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Football baseball basketball</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseball giants cubs redsox</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>football broncos cowboys</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseball redsox tigers</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop stars hendrix prince</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hendrix prince jagger rock</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joplin pearl jam tupac rock</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              component_1  component_2\n",
       "Football baseball basketball          1.0         -0.0\n",
       "baseball giants cubs redsox           1.0          0.0\n",
       "football broncos cowboys              1.0          0.0\n",
       "baseball redsox tigers                1.0         -0.0\n",
       "pop stars hendrix prince              0.0          1.0\n",
       "hendrix prince jagger rock            0.0          1.0\n",
       "joplin pearl jam tupac rock           0.0          1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dtm_svd.round(5),\n",
    "             index=example, \n",
    "             columns=[\"component_1\",\"component_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have reduced a 19 dimensional space corresponding to 19 unique words down to 2 dimensions. Similar docs point in similar directions. Dissimilar docs have perpendicular (orthogonal) vectors.\n",
    "\n",
    "Suppose we have a new sports-related document that is not already in our corpus, such as \"baseball basketball broncos\" and we want to see if it is more similar to component_1 or component_2. We will calculate the dot product of the new vector with each of the components and see which dot product is larger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product with component 1:  0.9659800000000001\n",
      "Dot product with component 2:  0.0\n"
     ]
    }
   ],
   "source": [
    "new_article = [1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # corresponds to \"baseball basketball broncos\" in the matrix above\n",
    "print(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\n",
    "print(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the document was much more similar to component 1. What about a new music-related document like \"jagger jam joplin\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product with component 1:  0.0\n",
      "Dot product with component 2:  0.54435\n"
     ]
    }
   ],
   "source": [
    "new_article = [0,0,0,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,0] # corresponds to \"jagger jam joplin\" in the matrix above\n",
    "print(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\n",
    "print(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about an article that is a combination of the two, such as \"redsox hendrix\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product with component 1:  0.47627\n",
      "Dot product with component 2:  0.51977\n"
     ]
    }
   ],
   "source": [
    "new_article = [0,0,0,0,0,0,0,1,0,0,0,0,0,0,1,0,0,0,0] # corresponds to \"redsox hendrix\" in the matrix above\n",
    "print(\"Dot product with component 1: \", np.dot(svd.components_[0].round(5), new_article))\n",
    "print(\"Dot product with component 2: \", np.dot(svd.components_[1].round(5), new_article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

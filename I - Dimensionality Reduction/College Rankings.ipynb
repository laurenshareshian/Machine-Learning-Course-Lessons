{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Algebra Python Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Let's see what the very important numpy and linalg packages can help us with. We can create matrices and multiply them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3],\n",
       "        [6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.matrix([[3,0],[8,-1]])\n",
    "b = np.matrix([[1],[2]])\n",
    "A*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.We can calculate the inverse, tranpose, and the determinant of a matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.33333333e-01  6.93889390e-18]\n",
      " [ 2.66666667e+00 -1.00000000e+00]]\n",
      "\n",
      "[[ 3  8]\n",
      " [ 0 -1]]\n",
      "\n",
      "-3.0\n"
     ]
    }
   ],
   "source": [
    "#inverse:\n",
    "print(linalg.inv(A))\n",
    "\n",
    "print()\n",
    "\n",
    "#transpose:\n",
    "print(A.T)\n",
    "\n",
    "print()\n",
    "\n",
    "#determinant:\n",
    "print(linalg.det(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.We can solve the system $Ax=b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333],\n",
       "       [0.66666667]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.We can find the least squares solution and projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.66666667]\n",
      " [0.5       ]]\n",
      "\n",
      "[[1.16666667]\n",
      " [1.66666667]\n",
      " [2.16666667]]\n"
     ]
    }
   ],
   "source": [
    "#least squares\n",
    "A=np.matrix([[1,1],[1,2],[1,3]])\n",
    "b=np.matrix([[1],[2],[2]])\n",
    "\n",
    "X=linalg.inv(A.T*A)*A.T*b\n",
    "print(X) # solution\n",
    "print()\n",
    "print(A*X) # projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.We can also calculate the eigenvalues and eigenvectors of a matrix, which we'll get to more later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.+0.j  3.+0.j]\n",
      "(array([-1.+0.j,  3.+0.j]), array([[0.        , 0.4472136 ],\n",
      "       [1.        , 0.89442719]]))\n"
     ]
    }
   ],
   "source": [
    "A=np.matrix([[3,0],[8,-1]])\n",
    "\n",
    "#eigenvalues:\n",
    "print(linalg.eigvals(A))\n",
    "\n",
    "#eigenvalues and eigenvectors:\n",
    "print(linalg.eig(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above output is read as $ \\lambda_1 = -1, \\lambda_2 = 3$ with eigenvectors $v_1 = [0,1],v_2=[0.447,0.894]$, which are the normalized versions of what you get from obtaining $v_1 = [0,1],v_2=[1,2]$ by hand.\n",
    "\n",
    "I must admit that the output you get from typing ```eigenvectors(([[3,0],[8,-1]])``` into wolfram alpha is nicer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### College Rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Read in the US News & World Report 2013 College Rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>College</th>\n",
       "      <th>Score</th>\n",
       "      <th>Academic Reputation</th>\n",
       "      <th>Selectivity rank (lower = better)</th>\n",
       "      <th>SAT</th>\n",
       "      <th>Percent in Top 10 of HS</th>\n",
       "      <th>Acceptance Rate</th>\n",
       "      <th>Fac Resources Rank (lower = better)</th>\n",
       "      <th>Percent classes fewer than 20 students</th>\n",
       "      <th>Percentage classes greater than 50 students (lower = better)</th>\n",
       "      <th>Fac Student Ratio (lower = better)</th>\n",
       "      <th>Percent full time faculty</th>\n",
       "      <th>Graduation Retention rank (lower = better)</th>\n",
       "      <th>Freshmen Retention</th>\n",
       "      <th>Financial resources rank (lower = better)</th>\n",
       "      <th>Alumni Giving rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>williams</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>91</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>71</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amherst</td>\n",
       "      <td>98</td>\n",
       "      <td>92</td>\n",
       "      <td>5</td>\n",
       "      <td>1425.0</td>\n",
       "      <td>84</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swarthmore</td>\n",
       "      <td>96</td>\n",
       "      <td>91</td>\n",
       "      <td>6</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>84</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>middlebury</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>6</td>\n",
       "      <td>1385.0</td>\n",
       "      <td>86</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pomona</td>\n",
       "      <td>94</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>90</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bowdoin</td>\n",
       "      <td>93</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>1410.0</td>\n",
       "      <td>83</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wellesley</td>\n",
       "      <td>93</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>78</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>14</td>\n",
       "      <td>95</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>carlton</td>\n",
       "      <td>92</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>1415.0</td>\n",
       "      <td>78</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>haverford</td>\n",
       "      <td>91</td>\n",
       "      <td>83</td>\n",
       "      <td>2</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>94</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>claremont_mckenna</td>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>14</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>86</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>vassar</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>97</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>davidson</td>\n",
       "      <td>89</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>82</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>99</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>harvey_mudd</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>95</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>67</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>21</td>\n",
       "      <td>98</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>us_naval_academy</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>46</td>\n",
       "      <td>1270.0</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>washington_and_lee</td>\n",
       "      <td>88</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>1395.0</td>\n",
       "      <td>81</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>0.2</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>25</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hamilton</td>\n",
       "      <td>87</td>\n",
       "      <td>81</td>\n",
       "      <td>17</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>74</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>94</td>\n",
       "      <td>21</td>\n",
       "      <td>95</td>\n",
       "      <td>23</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>wesleyan</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>19</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>66</td>\n",
       "      <td>24</td>\n",
       "      <td>48</td>\n",
       "      <td>68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>97</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>29</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colby</td>\n",
       "      <td>84</td>\n",
       "      <td>81</td>\n",
       "      <td>25</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>61</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>69</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>14</td>\n",
       "      <td>95</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>colgate</td>\n",
       "      <td>84</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>67</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>64</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>smith</td>\n",
       "      <td>84</td>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>1320.0</td>\n",
       "      <td>61</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>66</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>97</td>\n",
       "      <td>35</td>\n",
       "      <td>92</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>us_military_academy</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>28</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bates</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>28</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>grinnell</td>\n",
       "      <td>83</td>\n",
       "      <td>86</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>62</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9</td>\n",
       "      <td>91</td>\n",
       "      <td>28</td>\n",
       "      <td>94</td>\n",
       "      <td>27</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>macalaster</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>28</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>bryn_mawr</td>\n",
       "      <td>81</td>\n",
       "      <td>83</td>\n",
       "      <td>39</td>\n",
       "      <td>1315.0</td>\n",
       "      <td>60</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>74</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>92</td>\n",
       "      <td>23</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>oberlin</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>68</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>70</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>94</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                College  Score  Academic Reputation  \\\n",
       "0              williams    100                   92   \n",
       "1               amherst     98                   92   \n",
       "2            swarthmore     96                   91   \n",
       "3            middlebury     94                   87   \n",
       "4                pomona     94                   87   \n",
       "5               bowdoin     93                   87   \n",
       "6             wellesley     93                   89   \n",
       "7               carlton     92                   88   \n",
       "8             haverford     91                   83   \n",
       "9     claremont_mckenna     90                   85   \n",
       "10               vassar     90                   88   \n",
       "11             davidson     89                   83   \n",
       "12          harvey_mudd     89                   89   \n",
       "13     us_naval_academy     88                   88   \n",
       "14   washington_and_lee     88                   78   \n",
       "15             hamilton     87                   81   \n",
       "16             wesleyan     86                   85   \n",
       "17                colby     84                   81   \n",
       "18              colgate     84                   83   \n",
       "19                smith     84                   85   \n",
       "20  us_military_academy     83                   83   \n",
       "21                bates     83                   83   \n",
       "22             grinnell     83                   86   \n",
       "23           macalaster     82                   83   \n",
       "24            bryn_mawr     81                   83   \n",
       "25              oberlin     81                   82   \n",
       "\n",
       "    Selectivity rank (lower = better)     SAT  Percent in Top 10 of HS  \\\n",
       "0                                   4  1420.0                       91   \n",
       "1                                   5  1425.0                       84   \n",
       "2                                   6  1440.0                       84   \n",
       "3                                   6  1385.0                       86   \n",
       "4                                   2  1460.0                       90   \n",
       "5                                   8  1410.0                       83   \n",
       "6                                  12  1390.0                       78   \n",
       "7                                  12  1415.0                       78   \n",
       "8                                   2  1400.0                       94   \n",
       "9                                  14  1390.0                       71   \n",
       "10                                 14  1395.0                       74   \n",
       "11                                 10  1360.0                       82   \n",
       "12                                  1  1500.0                       95   \n",
       "13                                 46  1270.0                       53   \n",
       "14                                  9  1395.0                       81   \n",
       "15                                 17  1390.0                       74   \n",
       "16                                 19  1390.0                       66   \n",
       "17                                 25  1335.0                       61   \n",
       "18                                 19  1350.0                       67   \n",
       "19                                 35  1320.0                       61   \n",
       "20                                 28  1260.0                       58   \n",
       "21                                 28  1340.0                       58   \n",
       "22                                 32     NaN                       62   \n",
       "23                                 19  1340.0                       70   \n",
       "24                                 39  1315.0                       60   \n",
       "25                                 19  1370.0                       68   \n",
       "\n",
       "    Acceptance Rate  Fac Resources Rank (lower = better)  \\\n",
       "0                17                                    3   \n",
       "1                13                                    7   \n",
       "2                15                                    7   \n",
       "3                18                                   17   \n",
       "4                14                                   20   \n",
       "5                16                                   14   \n",
       "6                31                                   12   \n",
       "7                31                                   16   \n",
       "8                25                                    5   \n",
       "9                14                                    4   \n",
       "10               23                                   20   \n",
       "11               28                                   15   \n",
       "12               22                                   18   \n",
       "13                7                                   24   \n",
       "14               18                                    2   \n",
       "15               27                                    6   \n",
       "16               24                                   48   \n",
       "17               29                                   20   \n",
       "18               29                                   29   \n",
       "19               45                                   20   \n",
       "20               27                                   43   \n",
       "21               27                                   43   \n",
       "22               51                                   29   \n",
       "23               35                                   35   \n",
       "24               46                                   27   \n",
       "25               30                                   43   \n",
       "\n",
       "    Percent classes fewer than 20 students  \\\n",
       "0                                       71   \n",
       "1                                       70   \n",
       "2                                       74   \n",
       "3                                       68   \n",
       "4                                       70   \n",
       "5                                       68   \n",
       "6                                       69   \n",
       "7                                       65   \n",
       "8                                       79   \n",
       "9                                       86   \n",
       "10                                      68   \n",
       "11                                      69   \n",
       "12                                      67   \n",
       "13                                      61   \n",
       "14                                      74   \n",
       "15                                      74   \n",
       "16                                      68   \n",
       "17                                      69   \n",
       "18                                      64   \n",
       "19                                      66   \n",
       "20                                      67   \n",
       "21                                      67   \n",
       "22                                      62   \n",
       "23                                      70   \n",
       "24                                      74   \n",
       "25                                      70   \n",
       "\n",
       "    Percentage classes greater than 50 students (lower = better)  \\\n",
       "0                                                 4.0              \n",
       "1                                                 2.0              \n",
       "2                                                 2.0              \n",
       "3                                                 1.0              \n",
       "4                                                 1.0              \n",
       "5                                                 1.0              \n",
       "6                                                 1.0              \n",
       "7                                                 1.0              \n",
       "8                                                 1.0              \n",
       "9                                                 2.0              \n",
       "10                                                0.3              \n",
       "11                                                0.0              \n",
       "12                                                2.0              \n",
       "13                                                0.0              \n",
       "14                                                0.2              \n",
       "15                                                1.0              \n",
       "16                                                5.0              \n",
       "17                                                2.0              \n",
       "18                                                2.0              \n",
       "19                                                5.0              \n",
       "20                                                3.0              \n",
       "21                                                3.0              \n",
       "22                                                0.3              \n",
       "23                                                1.0              \n",
       "24                                                3.0              \n",
       "25                                                3.0              \n",
       "\n",
       "    Fac Student Ratio (lower = better)  Percent full time faculty  \\\n",
       "0                                    7                         93   \n",
       "1                                    9                         94   \n",
       "2                                    8                         93   \n",
       "3                                    9                         94   \n",
       "4                                    8                         94   \n",
       "5                                   10                         93   \n",
       "6                                    8                         93   \n",
       "7                                    9                         97   \n",
       "8                                    8                         94   \n",
       "9                                    9                         94   \n",
       "10                                   8                         95   \n",
       "11                                  11                         99   \n",
       "12                                   8                         97   \n",
       "13                                   9                         94   \n",
       "14                                   9                         91   \n",
       "15                                   9                         94   \n",
       "16                                   9                         97   \n",
       "17                                  10                         93   \n",
       "18                                   9                         95   \n",
       "19                                   9                         97   \n",
       "20                                  10                         95   \n",
       "21                                  10                         95   \n",
       "22                                   9                         91   \n",
       "23                                  10                         89   \n",
       "24                                   8                         90   \n",
       "25                                   9                         96   \n",
       "\n",
       "    Graduation Retention rank (lower = better)  Freshmen Retention  \\\n",
       "0                                            1                  97   \n",
       "1                                            1                  98   \n",
       "2                                            4                  97   \n",
       "3                                           11                  96   \n",
       "4                                            1                  98   \n",
       "5                                            6                  96   \n",
       "6                                           14                  95   \n",
       "7                                            4                  97   \n",
       "8                                            6                  96   \n",
       "9                                           11                  96   \n",
       "10                                           6                  97   \n",
       "11                                           6                  96   \n",
       "12                                          21                  98   \n",
       "13                                          25                  97   \n",
       "14                                          14                  94   \n",
       "15                                          21                  95   \n",
       "16                                           6                  96   \n",
       "17                                          14                  95   \n",
       "18                                          14                  94   \n",
       "19                                          35                  92   \n",
       "20                                          14                  94   \n",
       "21                                          14                  94   \n",
       "22                                          28                  94   \n",
       "23                                          28                  94   \n",
       "24                                          47                  92   \n",
       "25                                          32                  94   \n",
       "\n",
       "    Financial resources rank (lower = better)  Alumni Giving rank  \n",
       "0                                           6                  58  \n",
       "1                                          10                  57  \n",
       "2                                           9                  46  \n",
       "3                                           3                  55  \n",
       "4                                           6                  43  \n",
       "5                                          14                  50  \n",
       "6                                          10                  46  \n",
       "7                                          27                  58  \n",
       "8                                          15                  44  \n",
       "9                                          21                  43  \n",
       "10                                         13                  33  \n",
       "11                                         37                  53  \n",
       "12                                         18                  33  \n",
       "13                                          1                  21  \n",
       "14                                         25                  46  \n",
       "15                                         23                  47  \n",
       "16                                         29                  49  \n",
       "17                                         29                  41  \n",
       "18                                         32                  40  \n",
       "19                                         21                  36  \n",
       "20                                         35                  46  \n",
       "21                                         35                  46  \n",
       "22                                         27                  40  \n",
       "23                                         41                  39  \n",
       "24                                         23                  40  \n",
       "25                                         37                  38  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/collegedata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Drop Grinnell since it does not contain SAT info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['College'] != 'grinnell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Save the Score column as series b and save the rest of the dataframe (except for the college and score columns) as A_dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['Score']\n",
    "A_dataframe = df.drop(columns=['College', 'Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Convert the dataframes to numpy matrices and then tranpose vector b so that the shapes are 25x14 and 25x1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 14) (25, 1)\n"
     ]
    }
   ],
   "source": [
    "A=np.matrix(A_dataframe)\n",
    "b=np.matrix(b)\n",
    "b = b.T\n",
    "print(A.shape, b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Apply the least squares transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=linalg.inv(A.T*A)*(A.T)*b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.Print the weights in decending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.4748330614760352, 'Freshmen Retention')\n",
      "(0.44402849910595643, 'Academic Reputation')\n",
      "(0.1568103639574019, 'Alumni Giving rank')\n",
      "(0.0834561380982426, 'Percent in Top 10 of HS')\n",
      "(0.06145716039604565, 'Fac Student Ratio (lower = better)')\n",
      "(0.045818800864619647, 'Selectivity rank (lower = better)')\n",
      "(0.024414250428624662, 'Percent classes fewer than 20 students')\n",
      "(0.014115335305337595, 'Percentage classes greater than 50 students (lower = better)')\n",
      "(-0.002214583741056597, 'SAT')\n",
      "(-0.022757846448289154, 'Percent full time faculty')\n",
      "(-0.029479732409627424, 'Acceptance Rate')\n",
      "(-0.06828671150853616, 'Graduation Retention rank (lower = better)')\n",
      "(-0.08629494142243521, 'Fac Resources Rank (lower = better)')\n",
      "(-0.10011751785238254, 'Financial resources rank (lower = better)')\n"
     ]
    }
   ],
   "source": [
    "categories = A_dataframe.columns                   # column names\n",
    "\n",
    "tuples = []                                        # create tuples containing the category weights and names\n",
    "for i in range(len(categories)):\n",
    "    tuples.append((X[i][0,0], categories[i]))\n",
    "    \n",
    "tuples.sort(reverse = True)                        # sort in decending order\n",
    "\n",
    "for i in range(len(categories)):                   # print the output\n",
    "    print(tuples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.What questions or observations do you have about the importance of these categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.Print Colby's actual score and predicted score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted ranking: 84.55517212864586\n",
      "colby actual ranking: [84]\n"
     ]
    }
   ],
   "source": [
    "#get colby's info\n",
    "colby = df[df['College'] == 'colby']\n",
    "\n",
    "#apply the least squares projection to colby's info\n",
    "colby = colby.drop(columns = ['College', 'Score'])\n",
    "colby = np.matrix(colby)\n",
    "colby\n",
    "\n",
    "projection = colby*X\n",
    "\n",
    "#print the predicted and actual ranking\n",
    "print('predicted ranking:', projection[0,0])\n",
    "print('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average absolute error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Prediction Error: 0.49539386508238237\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for i in range(len(A)):\n",
    "    projection = A[i][:]*X\n",
    "    newerror = abs(projection[0,0] - b[i,0])\n",
    "    error = error + newerror\n",
    "    \n",
    "print('Average Absolute Prediction Error:', error/len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Discussion of Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAT Scores are on a much different scale than the other variables, so perhaps we should scale first? We have several options. We do NOT want to use the Standard scaler in this case because it causes the square matrix  $A^T A$ to have a determinant very close to zero, making it a nearly non-invertible matrix, which is bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.22950749562984e-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(A_dataframe)\n",
    "\n",
    "A = np.matrix(scaler.transform(A_dataframe))\n",
    "\n",
    "print(linalg.det(linalg.inv(A.T*A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try using the MinMax scaler instead, and this will no longer give us a zero determinat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.073530888128822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(A_dataframe)\n",
    "\n",
    "A = np.matrix(scaler.transform(A_dataframe))\n",
    "\n",
    "print(linalg.det(linalg.inv(A.T*A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it still does not give us predictions as good as the non-scaled version. For example, the weightings are wackier and Colby's prediction is worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90.08714553229467, 'Selectivity rank (lower = better)')\n",
      "(62.90344262517712, 'Percent in Top 10 of HS')\n",
      "(28.033586279227876, 'SAT')\n",
      "(16.834509195267742, 'Academic Reputation')\n",
      "(16.412458903052826, 'Percent classes fewer than 20 students')\n",
      "(9.908130978435764, 'Alumni Giving rank')\n",
      "(8.702879857101745, 'Fac Resources Rank (lower = better)')\n",
      "(7.902273414080135, 'Fac Student Ratio (lower = better)')\n",
      "(6.572495288934505, 'Financial resources rank (lower = better)')\n",
      "(-0.3363738170115411, 'Percent full time faculty')\n",
      "(-6.436559188506777, 'Acceptance Rate')\n",
      "(-6.44794516704593, 'Percentage classes greater than 50 students (lower = better)')\n",
      "(-11.645287688328125, 'Freshmen Retention')\n",
      "(-23.80914101286759, 'Graduation Retention rank (lower = better)')\n",
      "predicted ranking: 78.04071482316772\n",
      "colby actual ranking: [84]\n"
     ]
    }
   ],
   "source": [
    "X=linalg.inv(A.T*A)*(A.T)*b\n",
    "\n",
    "categories = A_dataframe.columns                   # column names\n",
    "\n",
    "tuples = []                                        # create tuples containing the category weights and names\n",
    "for i in range(len(categories)):\n",
    "    tuples.append((X[i][0,0], categories[i]))\n",
    "    \n",
    "tuples.sort(reverse = True)                        # sort in decending order\n",
    "\n",
    "for i in range(len(categories)):                   # print the output\n",
    "    print(tuples[i])\n",
    "    \n",
    "    \n",
    "#get colby's info\n",
    "colby = df[df['College'] == 'colby']\n",
    "\n",
    "#apply the least squares projection to colby's info\n",
    "colby = colby.drop(columns = ['College', 'Score'])\n",
    "\n",
    "\n",
    "\n",
    "colby = scaler.transform(colby)   # don't scale because it gives worse results\n",
    "colby = np.matrix(colby)\n",
    "colby\n",
    "\n",
    "projection = colby*X\n",
    "\n",
    "#print the predicted and actual ranking\n",
    "print('predicted ranking:', projection[0,0])\n",
    "print('colby actual ranking:', df[df['College'] == 'colby']['Score'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average error is also worse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Absolute Prediction Error: 3.3892794524637186\n"
     ]
    }
   ],
   "source": [
    "error = 0\n",
    "for i in range(len(A)):\n",
    "    projection = A[i][:]*X\n",
    "    newerror = abs(projection[0,0] - b[i,0])\n",
    "    error = error + newerror\n",
    "    \n",
    "print('Average Absolute Prediction Error:', error/len(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, when using Ordinary Least Squares to solve for the closed form solution, feature scaling will NOT be necessary; in fact, it may make our predictions worse.\n",
    "\n",
    "The exception is when you apply regularization (L1/L2 Ridge, Lasso, etc.) Then you should use feature scaling. However, in the above examples, we have not applied any regularization.\n",
    "\n",
    "In the past, we used feature scaling for gradient descent in order to help the solution converge in a shorter period of time.\n",
    "\n",
    "Speaking of gradient descent..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares vs. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to solve for an optimal regression solution. You can solve it via the analytical solution (OLS) or via an iterative algorithm such as gradient descent.\n",
    "\n",
    "1.**Similarities** between the two methods:\n",
    "\n",
    "- Both can be applied to linear regression models.\n",
    "- Both are minimizing the sum of the squared residuals $$\\sum_{i=1}^n(y^{(i)}_{\\text{predicted}}-y^{(i)}_{\\text{actual}})^2$$\n",
    "- Both work for multivariate problems.\n",
    "\n",
    "2.**Differences** between the two methods:\n",
    "\n",
    "OLS directly calculates the solution by solving for the system of equations generated when setting all partial derivatives = 0. The whole process is analytical and generates a closed form solution.\n",
    "\n",
    "In contrast, gradient descent starts from guessing the local min and proceeds by taking small steps along the direction of the steepest descent. It's numerical and iterative and when the step size is small enough, one expects the updated guess to approach the real local min (converge to the least squared solution). In addition, gradient descent is able to tackle a wider array of problems that are analytically unsolvable.\n",
    "\n",
    "The OLS closed-form solution may (should) be preferred for “smaller” datasets – if computing (a “costly”) matrix inverse is not a concern. For very large datasets, or datasets where the inverse of $X^T X$ may not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity), the GD or SGD approaches are to be preferred. Here's a good article going into more detail...\n",
    "\n",
    "https://sebastianraschka.com/faq/docs/closed-form-vs-gd.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

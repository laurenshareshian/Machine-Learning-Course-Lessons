{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1\n",
    "\n",
    "Recall our standard Gradient Descent (aka batch) algorithm:\n",
    "![image.png](images/sgd1.png)\n",
    "In the above algorithm says, to perform the GD, we need to calculate the gradient of the cost function J. And to calculate the gradient of the cost function, we need to sum (yellow circle!) the cost of each sample. If we have 3 million samples, we have to loop through 3 million times or use the dot product.\n",
    "\n",
    "Another downside of standard gradient descent is you have no guarantee that you will converge to the absolute minimum.\n",
    "\n",
    "An algorithm that will be faster (and has a higher likelihood of converging to the true minimum) is stochastic gradient descent, in which we introduce some randomness:\n",
    "![image.png](images/sgd7.png)\n",
    "![image.png](images/sgd6.png)\n",
    "Basically, in SGD, we are using the cost gradient of 1 example at each iteration, instead of using the sum of the cost gradient of ALL examples.\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "a) In SGD, before for-looping, you need to randomly shuffle the training examples.\n",
    "\n",
    "b) In SGD, because it’s using only one example at a time, its path to the minima is noisier (more random) than that of the batch gradient. But it’s ok as we are indifferent to the path, as long as it gives us the minimum AND the shorter training time. Here’s a picture to view the difference:\n",
    "![image.png](images/sgd3.png)\n",
    "Or in 3 D:![image.png](images/sgd4.png)\n",
    "\n",
    "c) Because we are only using one point at each iteration, we no longer need to take an average, and thus we will not be dividing by $m$ in our formula.\n",
    "\n",
    "Some pandas notes to help:\n",
    "1.\tTo shuffle the dataframe and reset the index:\n",
    "\n",
    "```\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "2.\tTo randomly select one row from an entire dataframe, use:\n",
    "\n",
    "```\n",
    "df.sample() \n",
    "```\n",
    "\n",
    "Or, to randomly select an integer between 0 and n, inclusive, put “import random” in your list of important packages and then type: \n",
    "```\n",
    "random.randint(0,n)\n",
    "```\n",
    "\n",
    "### Exercise 1:\n",
    "Get the SGD algorithm working. To test it, an input of sgd(X,Y,.01,0.00001) gave me out an output of b:86.84622507560407, m:-1.9109285548469945, (although yours will be different due to the randomization).\n",
    "\n",
    "### Exercise 2: \n",
    "Once you have the algorithm working, create a 2D (alpha0, alpha1) plot of both the GD and SGD algorithms. You will see the SGD algorithm take a more meandering path. To speed up the plotting, you will probably only want to plot every 100th point. (You can do this by creating a separate j counting variable and only plot the point if j % 100 == 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

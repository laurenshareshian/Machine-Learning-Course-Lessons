{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to import the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "---\n",
    "<a class=\"anchor\" id=\"train\"></a>\n",
    "\n",
    "The previous lessons were meant to get you comfortable with different types of machine learning algorithms. However, in practice, we would never use our entire dataset to train our model. Instead, we would use a portion of our data, the training set, to train the data, and then we would evaluate the accuracy of our model on the testing portion of our dataset. Since the test portion was not used to train the model, it gives us a more honest indication of how well our model does at predicting new data it hasn't encountered before. There are a few techniques for training and testing. This first one is less computationally intensive.\n",
    "\n",
    "### Technique 1: Train/Validate/Test\n",
    "\n",
    "First, a couple of vocab words:\n",
    "\n",
    "**Training Dataset**: The sample of data used to fit the model. The actual dataset that we use to train the model. The model sees and learns from this data.\n",
    "\n",
    "**Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters.  The validation set is used to evaluate a given model, but this is for frequent evaluation. We as machine learning engineers use this data to fine-tune the model hyperparameters. Hence the model occasionally sees this data, but never does it “Learn” from this. We use the validation set results and update higher level hyperparameters. So the validation set in a way affects a model, but indirectly.\n",
    "\n",
    "**Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset. The Test dataset provides the gold standard used to evaluate the model. It is only used once a model is completely trained (using the train and validation sets). The test set is generally what is used to evaluate competing models (For example on many Kaggle competitions, the validation set is released initially along with the training set and the actual test set is only released when the competition is about to close, and it is the result of the the model on the Test set that decides the winner).\n",
    "\n",
    "<img src=\"images/train.png\" width=\"400\">\n",
    "\n",
    "Side note: there is no hard and fast rule about how to proportion your data. Just know that your model is limited in what it can learn if you limit the data you feed it. However, if your test set is too small, it won’t provide an accurate estimate as to how your model will perform. \n",
    "\n",
    "### Boston Example\n",
    "\n",
    "Scikit-learn has many data sets built in that you can use. Let's load in a Boston dataset of housing info and housing prices. When you first load the data, it's a bit hard to understand what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "boston.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can read the docs to understand better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n",
      "Load and return the boston house-prices dataset (regression).\n",
      "\n",
      "    ==============     ==============\n",
      "    Samples total                 506\n",
      "    Dimensionality                 13\n",
      "    Features           real, positive\n",
      "    Targets             real 5. - 50.\n",
      "    ==============     ==============\n",
      "\n",
      "    Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    data : Bunch\n",
      "        Dictionary-like object, the interesting attributes are:\n",
      "        'data', the data to learn, 'target', the regression targets,\n",
      "        'DESCR', the full description of the dataset,\n",
      "        and 'filename', the physical location of boston\n",
      "        csv dataset (added in version `0.20`).\n",
      "\n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "\n",
      "        .. versionadded:: 0.18\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "        .. versionchanged:: 0.20\n",
      "            Fixed a wrong data point at [445, 0].\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.datasets import load_boston\n",
      "    >>> boston = load_boston()\n",
      "    >>> print(boston.data.shape)\n",
      "    (506, 13)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)\n",
    "print(load_boston.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that all of the predictor variables are contained in data and the target variable (housing price) is contained in the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Test/Train sets\n",
    "Let's omit the validation set for now and focus on splitting into training and testing sets. If we save 30% for the testing set and run a simple linear regression, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7498408727584818\n",
      "Test: 0.6793346289873795\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Fit the model against the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model against the testing data\n",
    "print('Train:', model.score(X_train, y_train))\n",
    "print('Test:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above cell multiple times to notice the variation. Notice the R^2 value of the test (hold-out) set. Notice that model performance is usually a little lower on the test set. This is expected. In fact, this lower value is a much more accurate number to report as \"real world\" performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Cross Validation\n",
    "\n",
    "Cross validation assigns a certain percentage of the dataset to test data, and then does this multiple times. \n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on the training set and testing the analysis on the other subset. To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance.\n",
    "\n",
    "The upside of this method is it avoids unluckily sampling one unrepresentative set of test data. However, the downside is this method is computationally intensive. This method works great on small to medium-sized datasets. This is absolutely not the kind of thing you’d want to try on a massive dataset. \n",
    "\n",
    "<img src=\"images/train3.png\" width=\"400\">\n",
    "\n",
    "Not surprisingly, scikit-learn can help us do this using a method called cross_val_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.46030057 26.04862111 33.07413798 80.76237112 33.31360656]\n",
      "Average MSE:  37.13180746769922\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# scores output is negative, which is because\n",
    "# Scikit-learn uses negative mean squared error so that \n",
    "# scores always improve with higher values \n",
    "print(-scores)\n",
    "print('Average MSE: ', np.mean(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of a **train/test split:**\n",
    "\n",
    "- Runs K times faster than K-fold cross-validation\n",
    "- Simpler to examine the detailed results of the testing process\n",
    "\n",
    "Advantages of **cross validation:**\n",
    "\n",
    "- More accurate estimate of out-of-sample accuracy\n",
    "- More \"efficient\" use of data (every observation is used for both training and testing)\n",
    "\n",
    "Recommendations for **cross validation:**\n",
    "\n",
    "- K can be any number, but K=4 or 5 is common\n",
    "\n",
    "- Each response class should be represented with equal proportions in each of the K folds\n",
    "\n",
    "- Scikit-learn's cross_val_score function does this by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "Source: https://medium.com/@contactsunny/why-do-we-need-feature-scaling-in-machine-learning-and-how-to-do-it-using-scikit-learn-d8314206fe73\n",
    "\n",
    "When you’re working with a learning model, it is important to scale the features to a range which is centered around zero. This is done so that the variance of the features are in the same range. If a feature’s variance is orders of magnitude more than the variance of other features, that particular feature might dominate other features in the dataset, which is not something we want happening in our model.\n",
    "The aim here is to to achieve Gaussian with zero mean and unit variance.\n",
    "\n",
    "The SciKit Learn library provides a class to easily scale our data. We can use the StandardScaler class from the library for this. Now that we know why we need to scale our features, let’s see how to do it. Consider the following dataset of consumer info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>72000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>48000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>54000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>61000</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>30000</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  income purchased\n",
       "0   44   72000        No\n",
       "1   27   48000       Yes\n",
       "2   30   54000        No\n",
       "3   38   61000        No\n",
       "4   40   30000       Yes"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[44,72000, 'No'], \n",
    "                        [27,48000, 'Yes'],\n",
    "                        [30,54000,'No'],\n",
    "                       [38,61000,'No'],\n",
    "                       [40,30000,'Yes'],\n",
    "                       [35,58000, 'Yes'],\n",
    "                       [48,79000,'Yes'],\n",
    "                       [50,83000,'No'],\n",
    "                      [18,0,'No'],\n",
    "                      [50, 100000, 'Yes']], columns = ['age', 'income', 'purchased'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create transform the categorical column purchased to a one-hot matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>72000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>48000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>54000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>61000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>30000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  income  No  Yes\n",
       "0   44   72000   1    0\n",
       "1   27   48000   0    1\n",
       "2   30   54000   1    0\n",
       "3   38   61000   1    0\n",
       "4   40   30000   0    1"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = pd.get_dummies(df['purchased'])\n",
    "df = df.drop('purchased', axis = 1)\n",
    "df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see now, the features are not at all on the same scale. We definitely need to scale them. Let’s look at the code for doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.593507</td>\n",
       "      <td>0.499777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.088096</td>\n",
       "      <td>-0.388716</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.791343</td>\n",
       "      <td>-0.166592</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.197836</td>\n",
       "      <td>-1.055085</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.296753</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.989178</td>\n",
       "      <td>0.758921</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.187014</td>\n",
       "      <td>0.907003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.978356</td>\n",
       "      <td>-2.165701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.187014</td>\n",
       "      <td>1.536352</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1    2    3\n",
       "0  0.593507  0.499777  1.0 -1.0\n",
       "1 -1.088096 -0.388716 -1.0  1.0\n",
       "2 -0.791343 -0.166592  1.0 -1.0\n",
       "3  0.000000  0.092551  1.0 -1.0\n",
       "4  0.197836 -1.055085 -1.0  1.0\n",
       "5 -0.296753 -0.018510 -1.0  1.0\n",
       "6  0.989178  0.758921 -1.0  1.0\n",
       "7  1.187014  0.907003  1.0 -1.0\n",
       "8 -1.978356 -2.165701  1.0 -1.0\n",
       "9  1.187014  1.536352 -1.0  1.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardScaler = StandardScaler()\n",
    "X = standardScaler.fit_transform(df)\n",
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after running this code with the dataset given above, we end up with a nicely scaled set of features as shown below. We can pass this input to a model and get much better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featuring Scaling and Train/Test Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data\n",
    "\n",
    "We should be careful to not apply scaling to the entire dataset at the beginning, though.\n",
    "\n",
    "In the interest of preventing information about the distribution of the test set leaking into your model, you should fit the scaler on your training data only, then standardise both training and test sets with that scaler. If instead you fitted the scaler on the full dataset prior to splitting, information about the test set is used to transform the training set, which in turn is passed downstream.\n",
    "\n",
    "As an example, knowing the distribution of the whole dataset might influence how you detect and process outliers, as well as how you parameterise your model. Although the data itself is not exposed, information about the distribution of the data is. As a result, your test set performance is not a true estimate of performance on unseen data. \n",
    "\n",
    "On our very small consumer example above, suppose we wanted to use our input data to predict the price of the item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>price_of_item</th>\n",
       "      <th>No</th>\n",
       "      <th>Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>72000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>48000</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>54000</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38</td>\n",
       "      <td>61000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40</td>\n",
       "      <td>30000</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  income  price_of_item  No  Yes\n",
       "0   44   72000           1000   1    0\n",
       "1   27   48000            100   0    1\n",
       "2   30   54000             50   1    0\n",
       "3   38   61000            100   1    0\n",
       "4   40   30000             20   0    1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[44,72000, 'No', 1000], \n",
    "                        [27,48000, 'Yes', 100],\n",
    "                        [30,54000,'No', 50],\n",
    "                       [38,61000,'No', 100],\n",
    "                       [40,30000,'Yes', 20],\n",
    "                       [35,58000, 'Yes', 1000],\n",
    "                       [48,79000,'Yes', 500],\n",
    "                       [50,83000,'No', 2],\n",
    "                      [18,0,'No', 50],\n",
    "                      [50, 100000, 'Yes', 1000]], columns = ['age', 'income', 'purchased', 'price_of_item'])\n",
    "\n",
    "one_hot = pd.get_dummies(df['purchased'])\n",
    "df = df.drop('purchased', axis = 1)\n",
    "df = df.join(one_hot)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to do an 80/20 train/test split. First, we would split the data and then apply feature scaling to our training set only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.042213</td>\n",
       "      <td>0.799185</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.240729</td>\n",
       "      <td>1.528875</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.935538</td>\n",
       "      <td>-1.945841</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-1.290994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.645179</td>\n",
       "      <td>0.555955</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-1.290994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049629</td>\n",
       "      <td>0.173736</td>\n",
       "      <td>1.290994</td>\n",
       "      <td>-1.290994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.042213</td>\n",
       "      <td>-0.277977</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.248146</td>\n",
       "      <td>-0.903426</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.248146</td>\n",
       "      <td>0.069494</td>\n",
       "      <td>-0.774597</td>\n",
       "      <td>0.774597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  1.042213  0.799185 -0.774597  0.774597\n",
       "1  1.240729  1.528875 -0.774597  0.774597\n",
       "2 -1.935538 -1.945841  1.290994 -1.290994\n",
       "3  0.645179  0.555955  1.290994 -1.290994\n",
       "4  0.049629  0.173736  1.290994 -1.290994\n",
       "5 -1.042213 -0.277977 -0.774597  0.774597\n",
       "6  0.248146 -0.903426 -0.774597  0.774597\n",
       "7 -0.248146  0.069494 -0.774597  0.774597"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['price_of_item']\n",
    "X = df.drop(columns=['price_of_item'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "X_train = standardScaler.fit_transform(X_train)\n",
    "pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we would apply our linear regression model. If we wanted to score our model on the testing data, we would then transform that separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5015772431344199\n",
      "Test: -529.9338203444653\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model against the testing data\n",
    "print('Train R^2:', model.score(X_train, y_train))\n",
    "\n",
    "X_test = standardScaler.fit_transform(X_test)\n",
    "\n",
    "print('Test R^2:', model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't be freaked if the test $R^2$ that you happen to get above is negative. This example was totally crappy and made up. The best possible R2 score is 1.0. A constant model that always predicts the expected value of y, disregarding the input features, would get an R^2 score of 0.0. An R^2 can be negative if the model is arbitrarily worse than just guessing the expected value of y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "Do a 70/30 train/test split and a cv=5 cross validation on your previous car dataset. Be sure to apply the StandardScalar to the training set first.\n",
    "\n",
    "How does the error compare to what you previously reported using the whole dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

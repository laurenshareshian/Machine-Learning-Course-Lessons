{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "    \n",
    "https://medium.com/diogo-menezes-borges/give-me-the-antidote-for-the-curse-of-dimensionality-b14bce4bf4d2\n",
    "\n",
    "Imagine you created a classifier that helps you distinguish üê± from üê∂ according to some features you gave, such as snout‚Äôs length, paw size, weight, color and type of fur. You have five features that in combination could be used by a classification algorithm to help you classify your samples. You start to think that maybe you can improve the classifier‚Äôs results if you just add more features based on other distinguishable characteristics. Maybe you can but probably you won‚Äôt. **As the number of feature or dimensions grows, the amount of data we need to generalise accurately grows exponentially.**\n",
    "\n",
    "<img src=\"images/dim1.png\" width=500>\n",
    "\n",
    "### Understanding Dimensionality\n",
    "\n",
    "Before going into details about the curse let‚Äôs understand the impact of dimensionality has on our datasets. Let‚Äôs imagine you have five boxes (dataset training observations). We want each one of these boxes to represent a part of that one-dimension, so we uniformly distribute them along a single line (feature X). At the moment, each one of these boxes owns an equal size subset (1/5) of this segment.\n",
    "\n",
    "<img src=\"images/dim2.png\" width=500>\n",
    "\n",
    "Now, let‚Äôs add another dimension (feature Y) and move into a two-dimensional space. It is desirable to maintain the same distance relationship between our data points, as in the one-dimension space. Since now we‚Äôre in a two-dimension space we must fill all empty spaces with more boxes so that we can maintain this distance between samples. In the end, we will end up with 25 data points available with each point occupying 1/25 of the space.\n",
    "\n",
    "From going from one-dimension to two-dimension we altered the total number of data points which was 5 into 25. What happens if we go into three-dimension space? In order to cover the whole dimension space, maintaining the same distance between points, we will end up having to find 125 data points ( 5¬≥ = boxes) with each point occupying 1/125 of the space. Therefore, the growth is exponential and everytime you increase the number of dimensions more boxes will have to be added to fill the empty spaces! Add another dimension and you‚Äôll get 645 boxes, another one and there‚Äôs 3125, six dimensions and you see 15625, and so forth.\n",
    "\n",
    "### The curse of dimensionality and overfitting\n",
    "\n",
    "In the previous example we saw that at every higher dimension the number of data points (boxes) in our space had to increase as well in order to maintain the same distance relationship between them. Usually, in real-life problem this is an issue. When you add a new feature to your model sometimes not enough data will be added to maintain previous relations and thus the new feature might not have a positive impact on the classifier. Therefore, you end up making it more complex without taking any advantage of it.\n",
    "\n",
    "Let‚Äôs go back to our cats and dogs example to understand this so-called curse. You start by building a classifier that only takes into account a single feature, for example, the snout‚Äôs length. The separation is not perfect but our classifier handles to classify each observation either dog or cat.\n",
    "\n",
    "<img src=\"images/dim3.png\" width=500>\n",
    "\n",
    "Nevertheless, we believe this classifier can do better! So, you decide to add a new feature (e.g. paw size). It becomes clear to us that only one feature wouldn‚Äôt cut it since we obtain better results with the introduction of the latter. However, with this feature, we still cannot make a perfect linear separation between our observations.\n",
    "\n",
    "<img src=\"images/dim4.png\" width=500>\n",
    "\n",
    "Following this line of thought, there‚Äôs a high probability our model‚Äôs classification capacity will increase if we add another feature. Let‚Äôs do that by adding the weight! Great! By adding the last feature our classifier is able to create a linear combination of the three features in order to obtain perfect classification results on our training dataset.\n",
    "\n",
    "<img src=\"images/dim5.png\" width=500>\n",
    "\n",
    "From this example, one would conclude that more features you had the better our classifier will perform, correct? WRONG! As shown by the first image on this post, there‚Äôs an optimal number of features to achieve. Above that, the classifier will lose performance. Moreover, note that the density of our training samples decreased exponentially as we increased the dimensionality of the problem which also can become a problem.\n",
    "\n",
    "\n",
    "As we add more features, the space between the observations grows, and it becomes sparser and sparser. This sparsity helps our classifier to classify our observations since it becomes more easy to find a separable hyperplane due to the fact that the likelihood of our training samples lying on the wrong side of the best hyperplane becomes infinitely small as you increase the number of features to infinite. However, by projecting this high dimensional classification into a lower dimensional space we are struck by an evident problem in Machine Learning: **Overfitting**!\n",
    "\n",
    "<img src=\"images/dim6.png\" width=500>\n",
    "\n",
    "Although our data was linearly separable in a three-dimensional space, this is not the case when we lower one dimension in the feature space. In fact, when we added the third feature the classifier learned the appearance of a specific instance and exceptions of our training dataset, hence overfitting and dooming our classifier to failure if tested with real-world data. Thus, we will get a better classifier by only using two features, since it will generalise better to any future dataset.\n",
    "\n",
    "<img src=\"images/dim7.png\" width=500>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another sparcity example\n",
    "\n",
    "We mentioned that s we add more features, the space between the observations grows, and it becomes sparser and sparser. If you need another example of this, think about the unit cube in $n$-dimensions. This is hard to visualize for $n>3$. The point of this code is to try to say something about this crazy, hard to visualize high-dimensional space thing. Algebraically, this is not too difficult to think about. It's just all the $n$-tuples of numbers between 0 and 1.\n",
    "\n",
    "#### How big is the unit cube?\n",
    "\n",
    "What is the length of the main-diagonal of the $n$-cube? In 2 dimensions, it's $\\sqrt 2$ and in 3 dimensions it's $\\sqrt 3$. In 100-dimensional space, the main diagonal is 10 units long. In 1M-dimensional space, the main diagonal is 1,000 units long. The 1M-dimensional unit cube has points that are pretty far apart from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding this curse\n",
    "\n",
    "Before going any further, have in mind there is no fixed rule that defines how many features should be used in a regression/classification problem. The magic number depends on the amount of training data available, the complexity of the decision boundaries and the type of classifier used. For example, if a theoretical number of training examples was available, such as in our example of the boxes we would not have to deal with this curse and we could simply use an infinite number of features to obtain the perfect classification.\n",
    "\n",
    "One main approache to reduce dimensionality is projection:\n",
    "\n",
    "### Projection\n",
    "\n",
    "As stated before, ‚Äúin most real-world problems training instances are not spread uniformly across all dimensions. You might have features that are constant while others are highly correlated which in the end makes all training instances lie within (or close to) a much lower-dimensional subspace of the high-dimensional space.‚Äù ‚Äî Hands-on Machine Learning with Scikit-Learn & TensorFlow.\n",
    "\n",
    "If we project every training instance from the below image b) perpendicularly onto to the 2D space we get the c) image. Notice that the axis for c) image are X and Y. Nevertheless, this approach is not always the best for dimensionality reduction. For example, the famous swiss roll shown below, is a good example why this approach is not good because by dropping the axis Z and projecting the instances in a 2D environment would squash different layers of the Swiss roll together (image c). The desired would be to unroll the Swiss toll to obtain the 2d image in d).\n",
    "\n",
    "<img src=\"images/dim8.png\" width=500>\n",
    "\n",
    "Examples of this approach are Singular Value Decomposition (SVD), Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA). We've already covered SVD and we'll cover PCA now.\n",
    "\n",
    "### Principal Component Analysis\n",
    "It starts by identifying the hyperplane that lies closest to the data and then it projects the data onto it.\n",
    "- **Maximizes Variance** : Finds direction/dimension of maximum variance of the data.\n",
    "- **Orthogonal** : Finds directions which are orthogonal (perpendicular to the first component that it found).\n",
    "\n",
    "Consider the following image:\n",
    "\n",
    "<img src=\"images/dim9.png\" width=500>\n",
    "\n",
    "We have on the left a representation of a simple 2D dataset with three 1D hyperplanes. On the other hand, on the right, the result of the projection of the dataset onto each one of those one-dimensional hyperplanes is illustrated. Looking at the three, it appears that the hyperplane represented by the solid line is the one that maximizes variance as desired. Therefore, the solid line is the most reasonable selection for a lower dimension. It also finds a second axis(dotted line), orthogonal to the first that accounts for the largest amount of remaining variance.\n",
    "\n",
    "\n",
    "If we were dealing with higher dimensions, PCA would find more orthogonal axis to the previous axes (for this case a third one if we‚Äôre dealing with 3D); as many axes as the number of dimensions in the dataset. The unit vector that defines the ith axis is called Principal Component (PC). In this case, the first PC is C1 and the second PC C2.\n",
    "\n",
    "### Projecting Down to d Dimensions\n",
    "\n",
    "Once we‚Äôve identified our principal components it is time to reduce the dimensionality of the dataset down to d dimensions by projecting it to the hyperplane defined by the first d principal components.\n",
    "\n",
    "\n",
    "Basically, we compute the dot product of the training dataset matrix X by the matrix Wd which contains the first d principal components. For this specific case Wd would be composed by the vectors C1 and C2.\n",
    "\n",
    "\n",
    "Usually a good number of dimensions is the one that adds up to a sufficiently large portion of the variance (~95%).\n",
    "\n",
    "### How is PCA related to SVD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix, C, can be computed for a data set with zero mean by calculating $Cov=\\frac{1}{n-1}X X^T$.\n",
    "\n",
    "Principal component analysis (PCA) is usually explained via an eigen-decomposition of the covariance matrix. \n",
    "\n",
    "Singular Value Decomposition (SVD) is a related decomposition that can be used to solve the PCA problem as well, and with better numeric properties.\n",
    "\n",
    "We can obtain the decomposition of the covariance matrix by performing singular value decomposition (SVD) on the data matrix ùêó:\n",
    "\n",
    "$$Cov(X) = \\frac{1}{n-1} X^TX$$\n",
    "\n",
    "$$X = U\\Sigma V^T$$\n",
    "\n",
    "$$Cov(X) = Cov(U\\Sigma V^T)$$\n",
    "\n",
    "$$\\frac{1}{n-1}X^TX = U\\Sigma V^T V \\Sigma U^T$$  but $$V^T V = I$$\n",
    "\n",
    "So $$X^TX = U \\frac{\\Sigma^2}{n-1} U^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of SVD & PCA\n",
    "\n",
    "\n",
    "### 1.EigenFaces\n",
    "\n",
    "In the last chapter, we explored image compression. If SVD is applied to a bunch of face shots, we get eigenfaces. Which look creepy, but are really useful for facial detection.\n",
    "\n",
    "![](http://archive.cnx.org/resources/28b7669c052b1d7ec07962bb69aa5cc3733eb868/PCA_Face.png)\n",
    "\n",
    "### 2.Google's PageRank\n",
    "\n",
    "PageRank is SVD on a markov chain. It helps Google figure out which items to give you to you when you search for something.\n",
    "\n",
    "### 3.Document Similarity using LSA\n",
    "\n",
    "Latent Semantic Analysis is just SVD applied to a word/document matrix. It will help us to figure out which types of documents are similar and pick out the document's most important themes. We'll get to this in our next chapter on Natural Language Processing.\n",
    "\n",
    "### 4.Recommender Systems\n",
    "Recommendation systems can be thought of as applying SVD to an User/Item matrix. We'll start on this tomorrow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
